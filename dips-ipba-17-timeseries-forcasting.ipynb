{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30530,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2023-11-06T05:22:21.596593Z","iopub.execute_input":"2023-11-06T05:22:21.599865Z","iopub.status.idle":"2023-11-06T05:22:21.628106Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/html":"","text/markdown":"","text/latex":"","text/plain":"character(0)"},"metadata":{}}]},{"cell_type":"code","source":"Time series Forcasting Kaggle Competation.\n\nStep 1- Install Packages\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\nlibrary(forecast)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tseries)\nlibrary(zoo)\nlibrary(prophet)\n\n\n\n# Import Train and Test Data\n\ntrain_data <- read.csv('/kaggle/input/ipba-17-time-series-forecasting-png/train.csv')\ntest_data <- read.csv('/kaggle/input/ipba-17-time-series-forecasting-png/test.csv')\nholidays <- read.csv('/kaggle/input/ipba-17-time-series-forecasting-png/holidays.csv')\n\n\n# Summary Statistics\nhead(train_data);dim(train_data)\nhead(test_data);dim(test_data)\nhead(holidays);dim(holidays)\nsummary(train_data);summary(test_data);summary(holidays)\n\n# Data Cleaning & Prep - Convert String to Date format code\n\nstr(train_data$date)\nstr(test_data$date)\nstr(holidays$Date)\n\n#Convert String to Date format code\ntrain_data$date = as.Date(train_data$date)\ntest_data$date = as.Date(test_data$date)\nholidays$Date = as.Date(holidays$Date)\nnames(holidays$Date)<-c('date','holidays')\nclass(train_data$date);class(test_data$date);class(holidays$Date)\nstr(train_data$date)\nstr(test_data$date)\nstr(holidays$Date)\n\n\n# Data Cleaning & Prep - Identify and Treat Missing Dates with mean of previous years\n# Sort the data by date\ntrain_data <- train_data %>% arrange(date)\ntest_data <- test_data %>% arrange(date)\n\nnrow(train_data)\nsum(is.na(train_data)) \n\nnrow(test_data)\nsum(is.na(test_data)) \n\n# Check for missing values in the date column of train_data and count them\nmissing_train_data <- sum(is.na(train_data$date))\n\n# Check for missing values in the date column of test_data and count them\nmissing_test_data <- sum(is.na(test_data$date))\n\n# Print the number of missing values in each data frame\ncat(\"Missing values in train_data:\", missing_train_data, \"\\n\")\ncat(\"Missing values in test_data:\", missing_test_data, \"\\n\")\n\n\n#After Date conversion\nhead(holidays)\nhead(train_data)\nhead(test_data)\n\n# Data Cleaning & Prep - Identify and Treat Outliers (tsclean)\ntrain_ts = ts(train_data$transactions, frequency = 365, start = c(2013, 1))\ncleaned_data = tsclean(train_ts)\nplot(train_ts)\n\n\n# EDA - STL plot (periodic)\nstl_result <- stl(cleaned_data, s.window = \"periodic\")\nplot(stl_result)\n\n# EDA - decompose plot (additive)\ndecomp_result <- decompose(cleaned_data, type = \"additive\")\nplot(decomp_result)\n\n\n# ACF and PACF Plots\nacf(cleaned_data)\npacf(cleaned_data)\n\n\n# Perform ADF Test -Augmented Dickey-Fuller Test\nadf_result <- adf.test(cleaned_data)\nsummary(adf_result)\nprint(adf_result)\n\n# ARIMA Model\nmodel_arima <- arima(cleaned_data, order = c(1,0,1))\nsummary(model_arima)\n\n# MA term ? - ACF plot \npar(mfrow=c(1, 3))\nacf(cleaned_data,lag.max =20) # Mostly all the correlated time period will be in last 20 time frame.\nacf(cleaned_data,lag.max =200)\nacf(cleaned_data,lag.max =400)\npar(mfrow=c(1, 1))\n\n\n# AR term - PACF plot\npar(mfrow=c(1, 3))\npacf(cleaned_data, lag.max = 20)\npacf(cleaned_data, lag.max = 200)\npacf(cleaned_data, lag.max = 400)\npar(mfrow=c(1, 1))\n\n\n# Check for I term - to check if time series is stationary or not \nadf.test(cleaned_data)\n# p-value is greater than 0.05 => TS in not stationary\nI =0\n#ARIMA = ( ?, 0, ?)\n\n\n\n# ARIMAX Model - with auto.arima (Holiday)\nmodel <- auto.arima(cleaned_data)\nsummary(model)\n\n# Other Model\n\n# ses model\n# Fit a Simple Exponential Smoothing (SES) model to the training data\nmod_ses <- ses(cleaned_data)\n\n# Make forecasts for the test data\npred_ses<-forecast(mod_ses, length(test_data))\n\n# Calculate accuracy metrics for the fitted values\naccuracy(mod_ses$fitted, cleaned_data)  # Replace 'cleaned_data' with your training data\n\n# Print the forecasted values\nprint(pred_ses)\n# Plot the forecasted values\nplot(pred_ses)\n\n\n# holt model \n# Fit a Holt's Exponential Smoothing (Holt) model\nmod_holt <- holt(cleaned_data)\n\n# Make forecasts for the test data\npred_holt <- forecast(mod_holt, h = length(test_data))\n\n# Calculate accuracy metrics for the fitted values\naccuracy(mod_holt$fitted, cleaned_data)  # Replace 'cleaned_data' with your training data\n\n# Print the forecasted values\nprint(pred_holt)\n\n# Plot the forecasted values\nplot(pred_holt)\n\n\n# holt winter model \nmod_hw<-HoltWinters(cleaned_data)\npred_hw<-forecast(mod_hw,h=length(test_data))\n\n# Calculate accuracy metrics for the fitted values\naccuracy(mod_hw$fitted, cleaned_data)  # Replace 'cleaned_data' with  training data\n\n# Print the forecasted values\nprint(pred_hw)\n# Plot the forecasted values\nplot(pred_hw)\n\n\n# Calculate Performance Metrics\n# 1 point for (ME, RMSE, MAE, MPE, MAPE)\nmodel <- auto.arima(cleaned_data)\nsummary(model)\n\n\n# ARIMAX Model - with auto.arima (Holiday)\n# Rename the 'Date' column in the 'holidays' dataset to 'date'\nnames(holidays)[names(holidays) == \"Date\"] <- \"date\"\n\n# Merge the cleaned_data with holidays\nmerged_data <- merge(cleaned_data, holidays, by.x = \"date\", by.y = \"date\", all.x = TRUE)\n\n# Replace NAs in the 'Holidays' column with 0 (assuming no holiday on those days)\nmerged_data$Holidays[is.na(merged_data$Holidays)] <- 0\n\n# Fit an ARIMAX model\nlibrary(forecast)\nmodel <- auto.arima(merged_data$transactions, xreg = merged_data$Holidays)\nsummary(model)\n\n\n\n# Forecast using Model\n# Replace NAs in the 'Holidays' column with 0 (assuming no holiday on those days)\nmerged_data$Holidays[is.na(merged_data$Holidays)] <- 0\n\n# Fit an ARIMAX model\nmodel <- auto.arima(merged_data$transactions, xreg = merged_data$Holidays)\n\n# Make forecasts\nforecast_horizon <- 12  # Adjust this value for your desired forecast horizon\nforecasts <- forecast(model, xreg = merged_data$Holidays, h = forecast_horizon)\n\n# Print the forecasts\nprint(forecasts)\n\n# Plot the forecasts\nplot(forecasts)\n\n\n# plot with Actual and Predicted Values\n# Sample data (replace with your own dataset)\nset.seed(42)\ndate_sequence <- seq.Date(from = as.Date(\"2023-01-01\"), by = \"days\", length.out = 100)\ndata <- data.frame(Date = date_sequence, Value = rnorm(100))\n\n# Split the data into training and testing sets (70% train, 30% test)\nset.seed(42)  # For reproducibility\nsplit_index <- floor(0.7 * nrow(data))\ntrain_data <- data[1:split_index, ]\ntest_data <- data[(split_index + 1):nrow(data), ]\n\n# Assuming you're using linear regression for forecasting\nmodel <- lm(Value ~ as.numeric(1:nrow(train_data)), data = train_data)\n\n# Forecast the testing data\nforecasted_values <- predict(model, newdata = data.frame(as.numeric(1:nrow(test_data))))\n\n# Create a subset of forecasted values to match the number of rows in test_data\nforecasted_values_subset <- forecasted_values[1:nrow(test_data)]\n\n# Add forecasted values to the test_data data frame\ntest_data$Forecasted <- forecasted_values_subset\n\n# Plot the actual vs. forecasted data\nlibrary(ggplot2)\n\nggplot(test_data, aes(x = Date)) +\n  geom_line(aes(y = Value, color = \"Actual\"), size = 1) +\n  geom_line(aes(y = Forecasted, color = \"Forecasted\"), size = 1, linetype = \"dashed\") +\n  labs(x = \"Date\", y = \"Value\", title = \"Actual vs. Forecasted Data\") +\n  scale_color_manual(values = c(\"Actual\" = \"green\", \"Forecasted\" = \"orange\")) +\n  theme_minimal()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:22:21.631813Z","iopub.execute_input":"2023-11-06T05:22:21.633657Z","iopub.status.idle":"2023-11-06T05:22:21.650913Z"},"trusted":true},"execution_count":10,"outputs":[{"ename":"ERROR","evalue":"Error in parse(text = x, srcfile = src): <text>:1:6: unexpected symbol\n1: Time series\n         ^\n","traceback":["Error in parse(text = x, srcfile = src): <text>:1:6: unexpected symbol\n1: Time series\n         ^\nTraceback:\n"],"output_type":"error"}]},{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n​\nlibrary(tidyverse) # metapackage of all tidyverse packages\n​\nlibrary(forecast)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tseries)\nlibrary(zoo)\nlibrary(prophet)\n​\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n​\nlist.files(path = \"../input\")\n​\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: Rcpp\n\nLoading required package: rlang\n\n\nAttaching package: ‘rlang’\n\n\nThe following objects are masked from ‘package:purrr’:\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\n\n'ipba-17-time-series-forecasting-png'\nadd Codeadd Markdown\ntrain_ds <- read.csv('/kaggle/input/ipba-17-time-series-forecasting-png/train.csv')\ntest_ds <- read.csv('/kaggle/input/ipba-17-time-series-forecasting-png/test.csv')\nholidays <- read.csv('/kaggle/input/ipba-17-time-series-forecasting-png/holidays.csv')\nadd Codeadd Markdown\n# Summary Statistics\nhead(train_ds);dim(train_ds)\nhead(test_ds);dim(test_ds)\nhead(holidays);dim(holidays)\nsummary(train_ds);summary(test_ds);summary(holidays)\nA data.frame: 6 × 2\ndate\ttransactions\n<chr>\t<int>\n1\t01-01-2013\t770\n2\t02-01-2013\t93215\n3\t03-01-2013\t78504\n4\t04-01-2013\t78494\n5\t05-01-2013\t93573\n6\t06-01-2013\t90464\n14552\nA data.frame: 6 × 2\ndate\tid\n<chr>\t<int>\n1\t01-01-2017\t0\n2\t02-01-2017\t1\n3\t03-01-2017\t2\n4\t04-01-2017\t3\n5\t05-01-2017\t4\n6\t06-01-2017\t5\n2272\nA data.frame: 6 × 2\nDate\tHolidays\n<chr>\t<int>\n1\t02-03-2012\t1\n2\t01-04-2012\t1\n3\t12-04-2012\t1\n4\t14-04-2012\t1\n5\t21-04-2012\t1\n6\t12-05-2012\t1\n3122\n     date            transactions   \n Length:1455        Min.   :   770  \n Class :character   1st Qu.: 75596  \n Mode  :character   Median : 80387  \n                    Mean   : 83491  \n                    3rd Qu.: 90166  \n                    Max.   :171169  \n     date                 id       \n Length:227         Min.   :  0.0  \n Class :character   1st Qu.: 56.5  \n Mode  :character   Median :113.0  \n                    Mean   :113.0  \n                    3rd Qu.:169.5  \n                    Max.   :226.0  \n     Date              Holidays    \n Length:312         Min.   :1.000  \n Class :character   1st Qu.:1.000  \n Mode  :character   Median :1.000  \n                    Mean   :1.122  \n                    3rd Qu.:1.000  \n                    Max.   :4.000  \nadd Codeadd Markdown\n#Data Cleaning & Preparation - Convert String to Date format code\nstr(train_ds$date)\nstr(test_ds$date)\nstr(holidays$Date)\n​\n#Convert String to Date format code\ntrain_ds$date = as.Date(train_ds$date)\ntest_ds$date = as.Date(test_ds$date)\nholidays$Date = as.Date(holidays$Date)\nnames(holidays$Date)<-c('date','holidays')\nclass(train_ds$date);class(test_ds$date);class(holidays$Date)\nstr(train_ds$date)\nstr(test_ds$date)\nstr(holidays$Date)\n chr [1:1455] \"01-01-2013\" \"02-01-2013\" \"03-01-2013\" \"04-01-2013\" ...\n chr [1:227] \"01-01-2017\" \"02-01-2017\" \"03-01-2017\" \"04-01-2017\" ...\n chr [1:312] \"02-03-2012\" \"01-04-2012\" \"12-04-2012\" \"14-04-2012\" ...\n'Date'\n'Date'\n'Date'\n Date[1:1455], format: \"1-01-20\" \"2-01-20\" \"3-01-20\" \"4-01-20\" \"5-01-20\" \"6-01-20\" \"7-01-20\" ...\n Date[1:227], format: \"1-01-20\" \"2-01-20\" \"3-01-20\" \"4-01-20\" \"5-01-20\" \"6-01-20\" \"7-01-20\" ...\n Date[1:312], format: \"2-03-20\" \"1-04-20\" \"12-04-20\" \"14-04-20\" \"21-04-20\" \"12-05-20\" ...\nadd Codeadd Markdown\n#Identifying and Treating Missing Dates with mean of previous years\n# Sort the data by date\ntrain_ds <- train_ds %>% arrange(date)\ntest_ds <- test_ds %>% arrange(date)\n​\nnrow(train_ds)\nsum(is.na(train_ds)) \n​\nnrow(test_ds)\nsum(is.na(test_ds)) \n​\n# Check for missing values in the date column of train_data and count them\nmissing_train_data <- sum(is.na(train_ds$date))\n​\n# Check for missing values in the date column of test_data and count them\nmissing_test_data <- sum(is.na(test_ds$date))\n​\n# Print the number of missing values in each data frame\ncat(\"Missing values in train_ds:\", missing_train_data, \"\\n\")\ncat(\"Missing values in test_ds:\", missing_test_data, \"\\n\")\n1455\n0\n227\n0\nMissing values in train_ds: 0 \nMissing values in test_ds: 0 \nadd Codeadd Markdown\n#After Date conversion\nhead(holidays)\nhead(train_ds)\nhead(test_ds)\nA data.frame: 6 × 2\nDate\tHolidays\n<date>\t<int>\n1\t2-03-20\t1\n2\t1-04-20\t1\n3\t12-04-20\t1\n4\t14-04-20\t1\n5\t21-04-20\t1\n6\t12-05-20\t1\nA data.frame: 6 × 2\ndate\ttransactions\n<date>\t<int>\n1\t1-01-20\t770\n2\t1-01-20\t1327\n3\t1-01-20\t2202\n4\t1-02-20\t78302\n5\t1-02-20\t100397\n6\t1-02-20\t98364\nA data.frame: 6 × 2\ndate\tid\n<date>\t<int>\n1\t1-01-20\t0\n2\t1-02-20\t31\n3\t1-03-20\t59\n4\t1-04-20\t90\n5\t1-05-20\t120\n6\t1-06-20\t151\nadd Codeadd Markdown\n#Identifying and Treating Outliers (tsclean)\ntrain_ts = ts(train_ds$transactions, frequency = 365, start = c(2013, 1))\ncleaned_data = tsclean(train_ts)\nplot(train_ts)\n\nadd Codeadd Markdown\n#EDA - STL plot (periodic)\nstl_result <- stl(cleaned_data, s.window = \"periodic\")\nplot(stl_result)\n\nadd Codeadd Markdown\n#EDA - decompose plot (additive)\ndecomp_result <- decompose(cleaned_data, type = \"additive\")\nplot(decomp_result)\n\nadd Codeadd Markdown\n#ACF Plots\nacf(cleaned_data)\n​\n#PACF Plots\npacf(cleaned_data)\n\n\nadd Codeadd Markdown\n#Performing ADF Test -Augmented Dickey-Fuller Test\nadf_result <- adf.test(cleaned_data)\nsummary(adf_result)\nprint(adf_result)\nWarning message in adf.test(cleaned_data):\n“p-value smaller than printed p-value”\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \nalternative 1      -none- character\np.value     1      -none- numeric  \nmethod      1      -none- character\ndata.name   1      -none- character\n\n\tAugmented Dickey-Fuller Test\n\ndata:  cleaned_data\nDickey-Fuller = -6.9201, Lag order = 11, p-value = 0.01\nalternative hypothesis: stationary\n\nadd Codeadd Markdown\n#ARIMA Model\nmodel_arima <- arima(cleaned_data, order = c(1,0,1))\nsummary(model_arima)\n\nCall:\narima(x = cleaned_data, order = c(1, 0, 1))\n\nCoefficients:\n         ar1      ma1   intercept\n      0.4503  -0.1197  83152.1873\ns.e.  0.0599   0.0648    394.8403\n\nsigma^2 estimated as 88541924:  log likelihood = -15377.13,  aic = 30762.27\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 2.341404 9409.672 7476.893 -1.179682 8.848857 0.8821084\n                     ACF1\nTraining set -0.002007772\nadd Codeadd Markdown\n# MA term ? - ACF plot \n​\npar(mfrow=c(1, 3))\nacf(cleaned_data,lag.max =20) # Mostly all the correlated time period will be in last 20 time frame.\nacf(cleaned_data,lag.max =200)\nacf(cleaned_data,lag.max =400)\npar(mfrow=c(1, 1))\n\nadd Codeadd Markdown\n# AR term - PACF plot\npar(mfrow=c(1, 3))\npacf(cleaned_data, lag.max = 20)\npacf(cleaned_data, lag.max = 200)\npacf(cleaned_data, lag.max = 400)\npar(mfrow=c(1, 1))\n\nadd Codeadd Markdown\n# Check for I term - to check if time series is stationary or not \nadf.test(cleaned_data)\n# p-value is greater than 0.05 => TS in not stationary\nI =0\nWarning message in adf.test(cleaned_data):\n“p-value smaller than printed p-value”\n\n\tAugmented Dickey-Fuller Test\n\ndata:  cleaned_data\nDickey-Fuller = -6.9201, Lag order = 11, p-value = 0.01\nalternative hypothesis: stationary\nadd Codeadd Markdown\n#ARIMAX Model - with auto.arima (Holiday)\nmodel <- auto.arima(cleaned_data)\nsummary(model)\nSeries: cleaned_data \nARIMA(4,1,5) \n\nCoefficients:\n          ar1     ar2      ar3      ar4      ma1      ma2     ma3     ma4\n      -0.2481  1.1183  -0.0584  -0.8344  -0.4964  -1.4414  0.8160  0.7541\ns.e.   0.0397  0.0324   0.0457   0.0358   0.0565   0.0277  0.1027  0.0313\n          ma5\n      -0.6035\ns.e.   0.0566\n\nsigma^2 = 79440365:  log likelihood = -15285.32\nAIC=30590.64   AICc=30590.79   BIC=30643.46\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE     MASE       ACF1\nTraining set 37.36595 8882.251 6808.448 -0.9846441 8.041626 0.597526 0.04088999\nadd Codeadd Markdown\n#Other Model - holt winter model ,ses model,holt model\n# holt winter model \nmod_hw<-HoltWinters(cleaned_data)\npred_hw<-forecast(mod_hw,h=length(test_ds))\n#accuracy(mod_hw$fitted, ts_train)\n​\n# Print the forecasted values\nprint(pred_hw)\n# Plot the forecasted values\nplot(pred_hw)\n          Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2016.9863       85581.41 70329.63 100833.2 62255.82 108907.0\n2016.9890       87895.10 72641.86 103148.3 64567.28 111222.9\n\nadd Codeadd Markdown\n# ses model\n# Fit a Simple Exponential Smoothing (SES) model to the training data\nmod_ses <- ses(cleaned_data)\n​\n# Make forecasts for the test data\npred_ses<-forecast(mod_ses, length(test_ds))\n​\n# Calculate accuracy metrics for the fitted values\naccuracy(mod_ses$fitted, cleaned_data)  # Replace 'cleaned_data' with your training data\n​\n# Print the forecasted values\nprint(pred_ses)\n# Plot the forecasted values\nplot(pred_ses)\nA matrix: 1 × 7 of type dbl\nME\tRMSE\tMAE\tMPE\tMAPE\tACF1\tTheil's U\nTest set\t36.69142\t9825.839\t7855.087\t-1.215635\t9.246583\t0.2964826\t0.856464\n          Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2016.9863       89721.95 77120.97 102322.9 70450.41 108993.5\n2016.9890       89721.95 77116.88 102327.0 70444.16 108999.7\n2016.9918       89721.95 77112.80 102331.1 70437.91 109006.0\n2016.9945       89721.95 77108.71 102335.2 70431.66 109012.2\n2016.9973       89721.95 77104.63 102339.3 70425.41 109018.5\n2017.0000       89721.95 77100.54 102343.4 70419.17 109024.7\n2017.0027       89721.95 77096.46 102347.4 70412.93 109031.0\n2017.0055       89721.95 77092.38 102351.5 70406.69 109037.2\n2017.0082       89721.95 77088.30 102355.6 70400.45 109043.5\n2017.0110       89721.95 77084.22 102359.7 70394.21 109049.7\n\nadd Codeadd Markdown\n# holt model \n# Fit a Holt's Exponential Smoothing (Holt) model\nmod_holt <- holt(cleaned_data)\n​\n# Make forecasts for the test data\npred_holt <- forecast(mod_holt, h = length(test_ds))\n​\n# Calculate accuracy metrics for the fitted values\naccuracy(mod_holt$fitted, cleaned_data)  # Replace 'cleaned_data' with your training data\n​\n# Print the forecasted values\nprint(pred_holt)\n​\n# Plot the forecasted values\nplot(pred_holt)\nA matrix: 1 × 7 of type dbl\nME\tRMSE\tMAE\tMPE\tMAPE\tACF1\tTheil's U\nTest set\t-237.7993\t10230.78\t8199.628\t-1.509617\t9.694957\t0.2613101\t0.9072103\n          Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2016.9863       107309.2 94179.83 120438.5 87229.58 127388.7\n2016.9890       107887.0 94582.46 121191.6 87539.46 128234.6\n","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:22:21.806988Z","iopub.execute_input":"2023-11-06T05:22:21.808683Z","iopub.status.idle":"2023-11-06T05:22:21.827582Z"},"trusted":true},"execution_count":11,"outputs":[{"ename":"ERROR","evalue":"Error in parse(text = x, srcfile = src): <text>:4:1: unexpected input\n3: # For example, here's a helpful package to load\n4: ​\n   ^\n","traceback":["Error in parse(text = x, srcfile = src): <text>:4:1: unexpected input\n3: # For example, here's a helpful package to load\n4: ​\n   ^\nTraceback:\n"],"output_type":"error"}]}]}